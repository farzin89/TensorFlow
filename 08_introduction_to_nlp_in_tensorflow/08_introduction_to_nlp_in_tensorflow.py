# -*- coding: utf-8 -*-
"""08_introduction_to_nlp_in_tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Chx_UraOYKH3saYq4BVxnh66Ih4SarB_

# Introduction to NLP Fundamentals in TensorFlow 

NLP has the goal of deriving information out of natural language (could be seqences text or speech).

Another common for NLP programs is sequence to sequence problems(seq2seq).

# Check for GPU
"""

!nvidia-smi -L

"""# Get helper function """

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py

# Import series of helper functions for the notebook

from helper_functions import unzip_data,create_tensorboard_callback,plot_loss_curves,compare_historys

"""## Get a tex dataset 

the dataset we're goinig to be using is Kaggle's introduction to NLP dataset (text samples of Tweets labelled as disaster or not disaster).

see the original source here: https://www.kaggle.com/c/nlp-getting-started/
"""

!wget https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip

# unzip data 
unzip_data("nlp_getting_started.zip")

"""## visualizing a text dataset 

To visualize our text samples, we first have to read them in, one way to do so would be to use Python: https://realpython.com/read-write-files-python/

But I prefer to get visual straight away.
So another way to do this is to use pandas...
"""

import pandas as pd
train_df = pd.read_csv("train.csv")
test_df = pd.read_csv("test.csv")
train_df.head()

train_df["text"][1]

# shuffle training dataframe
train_df_shuffled = train_df.sample(frac = 1, random_state = 42)
train_df_shuffled.head()

# what does the test datafram look like?
test_df.head()

# how many examples of each class ?
train_df.target.value_counts()

# how many total samples ?
len(train_df), len(test_df)

# Let's Visualize some random training examples
import random
random_index = random.randint(0,len(train_df)-5) #create random indexs not higher than the total number of samples
for row in train_df_shuffled[["text","target"]][random_index:random_index + 5].itertuples():
  _,text,target = row
  print(f"Target:{target}","(real disaster)" if target > 0 else "(not real disaster)")
  print(f"Text:\n{text}\n")
  print("---\n")

"""### Split data into training and validation sets"""

from sklearn.model_selection import train_test_split

# use train_test_split to split training data into training and validation sets
train_sentences,val_sentences,train_labels, val_labels = train_test_split(train_df_shuffled["text"].to_numpy(),
                                                                          train_df_shuffled["target"].to_numpy(),
                                                                          test_size = 0.1, # use 10% of training data for validation
                                                                          random_state = 42)

# check the lengths
len(train_sentences),len(train_labels),len(val_sentences),len(val_labels)

len(train_df_shuffled)

# check the first 10 samples
train_sentences[:10],train_labels[:10]

"""## Convert text into numbers """