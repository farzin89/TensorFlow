# -*- coding: utf-8 -*-
"""Deep learning and TensorFlow fundamentals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rlCf0jbK9TTR41ntX2x-X8KSwBMi3J4q
"""

import tensorflow as tf
print(tf.__version__)
import numpy as np

scalar = tf.constant(7)
scalar

scalar.ndim

#Creat a vector
vector = tf.constant([10,10])
vector

#check the dimension of our vector 
vector.ndim

#creat a matrix (has more than 1 dimention)
matrix = tf.constant([[10,7],
                      [7,10]])
matrix

matrix.ndim

#creat another matrix
another_matrix = tf.constant([[10.,7.],
                              [3.,2.],
                              [8.,9.]] , dtype = tf.float16) #specify the data type with dtype parameter
another_matrix

#what is the number dimentions of another_matrix?
another_matrix.ndim

#lets creat a tensor
tensor = tf.constant([[[1,2,3],
                       [4,5,6]],
                       [[7,8,9],
                        [10,11,12]],
                        [[13,14,15],
                        [16,17,18]]])
tensor

tensor.ndim

#what we have created so far:

# *scalar: a single number 
# *vector: a number with direction (e.g wind speed and direction)
# * matrix : a 2-dimentional array of numbers
#* Tensor : an n-dimentional array of numbers(whwn n can be any number,a 0-dimentional tensor is a scalar,a 1 dimentioal tensor is a vector)

#Creating tensors with tf.Variable 
tf.Variable

#Creat the same tensor with tf.Variable () as above

changeable_tensor = tf.Variable([10,7])
unchangeable_tensor = tf.constant([10,7])
changeable_tensor,unchangeable_tensor

#lets try to change one of the elements in our changeable tensor
changeable_tensor[0]= 7
changeable_tensor

# how about we try .assign()
changeable_tensor[0].assign(7)
changeable_tensor

# lets try change our unchanable tensor
unchangeable_tensor[0].assign(7)
unchangeable_tensor

# creating random trnsors

"""Crearing random tensors

random tensors are tensors of ability size which contain random numbers

"""



#ctreate two random(but the same) tensore
random_1 = tf.random.Generator.from_seed(7) #set seed for reproducibility
random_1 = random_1.normal(shape=(3,2))
random_2 = tf.random.Generator.from_seed(7)
random_2 = random_2.normal(shape=(3,2))
# are they equal?
random_1,random_2,random_1==random_2

"""###shuffle the order of elements in tensor"""

#shuffle a tensor (valuable for when you want to shuffle your data so the inherent order does not effect learning )
not_shufled = tf.constant([[10,7],
                           [3,4],
                           [2,5]])
#shuffle our non_shuffled tensor

tf.random.shuffle(not_shufled)

not_shufled

#shuffle our non_shuffled tensor

tf.random.shuffle(not_shufled)

#shuffle our non_shuffled tensor

tf.random.shuffle(not_shufled,seed=42)

"""::

**exercise:** Read through TensorFlow documentation on random seed generation:https://www.tensorflow.org/api_docs/python/tf/random/set_seed and practice writing 5 random tensors and shuffle them.

it looks like if we want our shuffled tensors to be in the same order, we have got to use the global level random seed as well as the operation level random seed

Rule 4 :"If both the global and the operation seed are set: Both seeds are used in conjunction to determine the random sequence.
"""

tf.random.set_seed(42) #global level random seed
tf.random.shuffle(not_shufled,seed=42) # operation level random seed

"""***other ways to make tensors

"""



# create a tensor of all ones
tf.ones([10,7])

#create a tensor of all zeroes
tf.zeros(shape=(3,4))

"""***Turn numpy array into tensors

the main difference between Numpy arrays and Tensorflow tensors is that tensors can be run on a GPU (much faster for numerical computing).

"""

# you can also turn numpy arrays into tensors
import numpy as np
numpy_A = np.arange(1, 25, dtype = np.int32) # create a Numpy array between 1 and 25
numpy_A 
# x = tf.constant(some_matrix) #capital for matrix or tensor
# y = tf.constant(vector) # non_caital for vector

A = tf.constant(numpy_A,shape = (2,3,4))
B = tf.constant(numpy_A)
A,B

A.ndim

"""###Geting information from tensor

when dealing with tensors you problably want to be aware of the following attributes:
* Shape
* Rank
* Axis or dimention
* Size

"""

# create a rannk 4 tensor (4 dimentions)
rank_4_tensor= tf.zeros(shape=[2,3,4,5])
rank_4_tensor

rank_4_tensor[0]

rank_4_tensor.shape,rank_4_tensor.ndim,tf.size(rank_4_tensor)

2*3*4*5

# Get variable attributes of our tensor

print("Datatype of every element:", rank_4_tensor.dtype)
print("Number of dimentions(rank):", rank_4_tensor.ndim)
print("shape of tensore:",rank_4_tensor.shape)
print("Elements along the 0 axis:", rank_4_tensor.shape[0])
print("Elements along the last axis:",rank_4_tensor.shape[-1])
print("Total number of elements in our tensor:",tf.size(rank_4_tensor))
print("Total number of elements in our tensor:",tf.size(rank_4_tensor).numpy())



"""### Indexing tensors

Tensors can be indexed just like Python lists.

"""

some_list = [1,2,3,4]
 some_list

#Get the first 2 elements of each dimention
rank_4_tensor[:2,:2,:2,:2]

some_list[:1]

rank_4_tensor.shape

#get the first element from each dimention from each index except for the final one

rank_4_tensor[:1,:1,:1,:]

# creat a rank 2 tensor(2 dimention)
rank_2_tensor = tf.constant([[10,7],
                             [3,4]])
rank_4_tensor.shape, rank_4_tensor.ndim

some_list,some_list[-1]

#get the last item of each of row of our rank 2
rank_2_tensor[:,-1]

# Add in extra dimention to our rank 2 tensor
rank_3_tensor = rank_2_tensor[...,tf.newaxis]
rank_3_tensor

# Alternative to tf.newaxis
tf.expand_dims(rank_2_tensor,axis= -1) # "-1" means expand the final axis

tf.expand_dims(rank_2_tensor,axis= 0 ) # expand the zero axis

rank_2_tensor

"""### Manipulating tensors (tensor operations)

**Basic operation**

'+' , '-','*','/'
"""

#you can add values to atensor using the addition opearator 
tensor = tf.constant([[10,7],[3,4]])
tensor + 10

# original tensor is unchanged
tensor

# multiplication also works
tensor * 10

# subtraction if you want
tensor - 10

# we can use the tensorflow built-in function too
tf.multiply(tensor,10)

tensor

"""**Matrix multiplication**

in machins learning, matrix multiplication is one of the most common tensor operation 
www.matrixmultiplication.xyz

There are two rules our tensors(or matricses) need to fulfil if we are going to matrix multiply them:      
1. The inner dimentions must match
2. The resulting matrix has the shape of the inner dimensions
"""

# matrix multiplication in tensorflow
 print(tensor)

tf.matmul(tensor,tensor)

# Matrix muliplication with python operator "@"
tensor @ tensor

tensor.shape

# creat a tensor (3,2) tensor
x = tf.constant([[1,2],
                 [3,4],
                 [5,6]])
# creat another tensor (3,2) tensor
y = tf.constant([[7,8],
                 [9,10],
                 [11,12]])
x,y

# Try to matrix multiply tensors of same shape
y@x

tf.matmul(x,y)

"""*** Resourse***
info and example of matrix multiplication www.mathsisfun.com/algebra/matrix-multiplying.html
"""

y

# Lets change the shape of Y
tf.reshape(y,shape=(2,3))

x.shape,tf.reshape(y,shape=(2,3)).shape

# Try to matrix multiply x by reshaped y 
x @ tf.reshape(y,shape=(2,3))

tf.matmul(x,tf.reshape(y,shape=(2,3)))

#Try to change the shape of x insted of y
tf.matmul(tf.reshape(x,shape=(2,3)),y)

# can do the same with transposs
x,tf.transpose(x), tf.reshape(x,shape=(2,3))

# Try matrix multiplication with transpose rather than reshape
tf.matmul(tf.transpose(x),y)

"""**The dot product**

Matrix multiplication is also referred to as the dot product.

you can perform matrix multiplication using :    
*' tf.matmul()
*' tf.tensordot()'

"""

x ,y

# perform the dot product on x and y (requires x or y to be transposed)

tf.tensordot(tf.transpose(x),y,axes=1)

# perform matrix multiplication between x and y(transposed)
tf.matmul(x,tf.transpose(y))

# perform matrix multiplication between x and y(reshaped)
tf.matmul(x,tf.reshape(y,shape=(2,3)))

# check the values of y,reshape y and transposed y
print("normal y:")
print(y,"\n") #:"\n" is for newline

print("y reshaped to (2,3):")
print(tf.reshape(y,(2,3)),"\n")

print("y transposed:")
print(tf.transpose(y))

tf.matmul(x,tf.transpose(y))

"""Generally, when performing matrix multiplication on two tensors and one of the axes does not line up,you will transpose(rather than reshape) one of the tensors to get satisfy the matrix multiplication ruls.

### changing the datatype of a tensor
"""

# creat new rensor with default datatype (float 32)

B = tf.constant([1.7,7.4])
B.dtype

c = tf.constant([7,10])
c,c.dtype

# change from float32 to float 16 (reduce percision)
D = tf.cast(B, dtype=tf.float16)
D , D.dtype

# change from int32 to float32 
E = tf.cast(c,dtype=tf.float32)
E

E_float16 = tf.cast(E,dtype=tf.float16)
E_float16



"""### Aggregating tensors###

Aggregatibg tensors = condensing them from multiple values down to a smaller amount of values.
"""

# Get the absolute values 

D = tf.constant([-7,-10])
D

tf.abs(D)

"""Lets go through the following forms of aggregation:    

* Get the minimum
* Get the maximum 
* Get the mean of a tensor
* Get the sum of a tensor
"""

# creat a random tensor with values between 0 and 100 of size 50

E = tf.constant(np.random.randint(0,100,size = 50 ))
E

tf.size(E) , E.shape, E.ndim

# find the minimum
tf.reduce_min(E)

# Find the maximum

tf.reduce_max(E)

# find the mean 
tf.reduce_mean(E)

# find the sum
tf.reduce_sum(E)

"""** Excercise** with what we have just learned , find the variance and standard deviation of our E tensor using Tensorflow methods.

"""

# find the variance of our tensor
tf.reduce_var(E)

# To find the variance of our tensor, we need access to tensorflow_probability 
import tensorflow_probability as tfp 
tfp.stats.variance(E)

# find the standard deviation 

tf.math.reduce_std(tf.cast(E, dtype = tf.float32))

# Find the variance of our E tensor 
tf.math.reduce_variance(tf.cast(E,dtype=tf.float32))

"""### find the positional maximum and minimum of a tensor"""

# Creat a new tensor for finding positional minimum and maximum.

F = tf.random.uniform(shape=[50]) # and also we can make with "tf.random.set_seed(42))"
F

# find the positional maximum 
tf.argmax(F)

# index on our largest value position
F[tf.argmax(F)]

# find the max value of F
tf.reduce_max(F)

# check for equality 
F[tf.argmax(F)]== tf.reduce_max(F)

# find the positional minimum 
tf.argmin(F)

#find the minimum using the positional minimum index

F[tf.argmin(F)]

"""### Squeezing a tensor (removing all single dimensions )

"""

# Create a tensor to get started
tf.random.set_seed(42)
g = tf.constant(tf.random.uniform(shape=[50]),shape = (1,1,1,1,50))
g

g.shape

g_squeezed = tf.squeeze(g)
g_squeezed, g_squeezed.shape

"""### one_hot encoding tensors"""

# Creat a list of indices

some_list = [0,1,2,3]  # could be red, green , blue, purple

# one hot encode our list of indices

tf.one_hot(some_list,depth= 4)

# Specify custom values for one hot encoding

tf.one_hot(some_list,depth=4,on_value="yo I love deep learning ", off_value="I also liketo dance")

"""### Squaring , log ,square root"""

# Creat a new tensor
H = tf.range(1,10)
H

#square it

tf.square(H)

# Find the squareroot (will error, method requires non_int type)

tf.sqrt(H)

# Find the squareroot 
tf.sqrt(tf.cast(H, dtype = tf.float32))

# find the log ( will error)
tf.math.log(H)

# Find the log

tf.math.log(tf.cast(H, dtype=tf.float32))

"""### Tensors and Numpy 

TensorFlow interacts beautifully with numpy array 

"""

# Creat a tensor directly from a numpy array

  j = tf.constant(np.array([3.,7.,10.]))
  j

# convert our tensor back to a numpy array 

np.array(j),type(np.array(j))

# Convert tensor j to a numpy array 
j.numpy(),type(j.numpy())

j = tf.constant([3.])
j.numpy()[0]

# The default types of each are slightly different
numpy_j = tf.constant(np.array([3.,7.,10.]))
tensor_j = tf.constant([3.,7.,10.])
# cheack the datatypes od each 
numpy_j.dtype,tensor_j.dtype

"""### Finding access to GPUs"""

tf.config.list_physical_devices()

"""*** Note:** If you have access to a CUDA-enable GPU, TensorFlow will automatically use it whenever possible ."""

