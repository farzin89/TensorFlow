# -*- coding: utf-8 -*-
"""05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18xH5I3cuJzatkEsf43LhBr8rNDcB1en2

# Teansfer learning with TensorFlow part 2 : Fine-tuning 

In the previous notebook, we covered transfer learning feature extraction, now it's time to learn about a new kind of transfer learning: fine-tuning.
"""

# check if we're using a GPU

!nvidia-smi

"""## Creating helper function 

in previous notebook we've created a bunch of helper function, now we could rewrite them all, however, this is tedious.

So, it's a good idea to put functions you'll want to use again in a script you can download and import into your notebooks (or elsewhere)

we've done this for same of the functions we've used previously here:
https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py
"""

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py

# Import helper functions we're going to use in this notebook 

from helper_functions import create_tensorboard_callback,plot_loss_curves,unzip_data, walk_through_dir

"""**Note:** If you're running this notebook in Google colab, when it times out colab will delete helper_function.py , so you have to redownload it if you want access to your helper functions.

## Let's get some data 

This time we're going to see how we can use the pretrained models within **tf.keras.applications** and apply them to our own problem (recognizing imsges of food). 
link : https://www.tensorflow.org/api_docs/python/tf/keras/applications
"""

# Get 10% of training data of 10 classes of Food101
 !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip

 unzip_data("10_food_classes_10_percent.zip")

# check out how many images and subdirectories are in our dataset
walk_through_dir("10_food_classes_10_percent")

# create training and test directory paths
train_dir = "10_food_classes_10_percent/train"
test_dir = "10_food_classes_10_percent/test"

import tensorflow as tf
IMG_SIZE = (224,224)
BATCH_SIZE = 32
train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory = train_dir,
                                                                            image_size = IMG_SIZE,
                                                                            label_mode ="categorical",
                                                                            batch_size = BATCH_SIZE)
test_data = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,
                                                                image_size = IMG_SIZE,
                                                                label_mode="categorical",
                                                                batch_size = BATCH_SIZE)

train_data_10_percent

# check out the class names of our dataset 
 train_data_10_percent.class_names

# See an example of a batch of data

for images,labels in train_data_10_percent.take(1):
  print(images,labels)
  
"""## Model 0 : Building a transfer learning feature extraction model using the keras functional API

The sequential API is straight-forward, it runs our layers in sequential order.

But the functional API gives us more flexibility with our model. https://www.tensorflow.org/guide/keras/functional

"""

# 1. Create base model with tf.keras.applications
base_model = tf.keras.applications.EfficientNetB0(include_top = False)

# 2. Freeze the base model(so the underlying pre-trains aren't updated during training )
base_model.trainable = False

# 3. Create inpute into our model
inputs = tf.keras.layers.Input(shape= (224,224,3),name="input_layer")

# 4. If using a model like ResNet50V2 you will need to normlize inputs(you don't have to for EfficientNet(s))
# x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)

# 5. pass the inputs to the base_model
 x = base_model(inputs)
 print(f"Shape after passing inputs through base model: {x.shape}")

 # 6. Average pool the outputs of the base model (aggregate all the most important information,reduce number of computations)
 x = tf.keras.layers.GlobalAveragePooling2D(name = "global_average_pooling_layer")(x)
 print(f"shape after GlobalAveragePooling2D: {x.shape}")

 # 7. Create the output activation layer
 outputs = tf.keras.layers.Dense(10,activation="softmax",name = "output_layer")(x)

 # 8 . combine the inputs with the outputs into a model
 model_0 = tf.keras.Model(inputs,outputs)

 # 9. compile for the model

 # 10. Fit the model
