# -*- coding: utf-8 -*-
"""05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18xH5I3cuJzatkEsf43LhBr8rNDcB1en2

# Teansfer learning with TensorFlow part 2 : Fine-tuning 

In the previous notebook, we covered transfer learning feature extraction, now it's time to learn about a new kind of transfer learning: fine-tuning.
"""

# check if we're using a GPU

!nvidia-smi

"""## Creating helper function 

in previous notebook we've created a bunch of helper function, now we could rewrite them all, however, this is tedious.

So, it's a good idea to put functions you'll want to use again in a script you can download and import into your notebooks (or elsewhere)

we've done this for same of the functions we've used previously here:
https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py
"""

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py

# Import helper functions we're going to use in this notebook 

from helper_functions import create_tensorboard_callback,plot_loss_curves,unzip_data, walk_through_dir

"""**Note:** If you're running this notebook in Google colab, when it times out colab will delete helper_function.py , so you have to redownload it if you want access to your helper functions.

## Let's get some data 

This time we're going to see how we can use the pretrained models within **tf.keras.applications** and apply them to our own problem (recognizing imsges of food). 
link : https://www.tensorflow.org/api_docs/python/tf/keras/applications
"""

# Get 10% of training data of 10 classes of Food101
 !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip

 unzip_data("10_food_classes_10_percent.zip")

# check out how many images and subdirectories are in our dataset
walk_through_dir("10_food_classes_10_percent")

# create training and test directory paths
train_dir = "10_food_classes_10_percent/train"
test_dir = "10_food_classes_10_percent/test"

import tensorflow as tf
IMG_SIZE = (224,224)
BATCH_SIZE = 32
train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory = train_dir,
                                                                            image_size = IMG_SIZE,
                                                                            label_mode ="categorical",
                                                                            batch_size = BATCH_SIZE)
test_data = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,
                                                                image_size = IMG_SIZE,
                                                                label_mode="categorical",
                                                                batch_size = BATCH_SIZE)

train_data_10_percent

# check out the class names of our dataset 
 train_data_10_percent.class_names

# See an example of a batch of data

for images,labels in train_data_10_percent.take(1):
  print(images,labels)
  
"""## Model 0 : Building a transfer learning feature extraction model using the keras functional API

The sequential API is straight-forward, it runs our layers in sequential order.

But the functional API gives us more flexibility with our model. https://www.tensorflow.org/guide/keras/functional

"""

# 1. Create base model with tf.keras.applications
base_model = tf.keras.applications.EfficientNetB0(include_top = False)

# 2. Freeze the base model(so the underlying pre-trains aren't updated during training )
base_model.trainable = False

# 3. Create inpute into our model
inputs = tf.keras.layers.Input(shape= (224,224,3),name="input_layer")

# 4. If using a model like ResNet50V2 you will need to normlize inputs(you don't have to for EfficientNet(s))
# x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)

# 5. pass the inputs to the base_model
 x = base_model(inputs)
 print(f"Shape after passing inputs through base model: {x.shape}")

 # 6. Average pool the outputs of the base model (aggregate all the most important information,reduce number of computations)
 x = tf.keras.layers.GlobalAveragePooling2D(name = "global_average_pooling_layer")(x)
 print(f"shape after GlobalAveragePooling2D: {x.shape}")

 # 7. Create the output activation layer
 outputs = tf.keras.layers.Dense(10,activation="softmax",name = "output_layer")(x)

 # 8 . combine the inputs with the outputs into a model
 model_0 = tf.keras.Model(inputs,outputs)

# 9. Compile the model
model_0.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(),
              metrics=["accuracy"])

# 10. Fit the model (we use less steps for validation so it's faster)
history_10_percent = model_0.fit(train_data_10_percent,
                                 epochs=5,
                                 steps_per_epoch=len(train_data_10_percent),
                                 validation_data=test_data,
                                 # Go through less of the validation data so epochs are faster (we want faster experiments!)
                                 validation_steps=int(0.25 * len(test_data)),
                                 # Track our model's training logs for visualization later
                                 callbacks=[create_tensorboard_callback("transfer_learning", "10_percent_feature_extraction")])

# Evaluate on the full test dataset
model_0.evaluate(test_data)

# Check the layers in our base model
for layer_number,layer in enumerate(base_model.layers):
  print(layer_number,layer.name)

# how about we get a summary of the model
base_model.summary()

# how about a summary of our whole model ?
model_0.summary()

# Check out our model's training curves
plot_loss_curves(history_10_percent)

"""## Getting a feature vector from a train model 

Let's demonstrate the Global Average pooling 2D layer...

We have a tensor after our model goes through 'base_model' of shape(None,7,7,1280).

But then when it passes through GlobalAveragePooling2D, it turns into (None, 1280)

Let's use a similar shaped tensor of (1,4,4,3) and then pass it to GlobalAveragePooling2D .

"""

# Define the inpot shape

input_shape = (1,4,4,3)

# Create a random tensor
tf.random.set_seed(42)
input_tensor = tf.random.normal(input_shape)
print(f"Random input tensor:\ {input_tensor}\n")

# Pass the random tensor through a global average pooling 2D layer
global_average_pooled_tensor = tf.keras.layers.GlobalAvgPool2D()(input_tensor)
print(f"2D global average pooled random tensor:\n {global_average_pooled_tensor}\n")

# Check the shape of the different tensors
print(f"shape of input tensore:{input_tensor.shape}")
print(f"shape of Glabal Average Pooled 2D tensor:{global_average_pooled_tensor.shape}")

# Let's replicate the GlobalAveragePool2D layer
tf.reduce_mean(input_tensor,axis=[1,2])

"""**practice:** Try to do the same with the above two cells but this time use 'GlobalMaxpool2D' and see what happens."""